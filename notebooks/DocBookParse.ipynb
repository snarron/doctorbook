{
 "metadata": {
  "name": "DocBookParse"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<h2>Dr. Book - Linking a Book to Open Data Sources</h2>\n",
      "<h3>Abstarct:</h3>\n",
      "<p>For any text-based resource, we will extract people, places, topics and concepts to render a webpage linking these resources to Open Data resources. We can parse books authored using the Docbook XML structure (http://docbook.org/tdg51/en/html/), which would provide more accuracy and relevance.</p>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<h3>Original Idea:</h3>\n",
      "<p>Two of our team members had been involved in Prof. Glushko's <i><a href=\"http://www.ischool.berkeley.edu/courses/i290-feb\">Future of eBooks</a></i> seminar in which students contributed to creating an <a href=\"www.futurepress.org\">open eBook reader</a>. We believed that books should be platform-independent and plugin-extensible.</p>\n",
      "<p>Our team decided to work on a project that would augment the reading experience by adding semantic web-like links that would connect information within books to the Web.</p>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#imports\n",
      "from bs4 import BeautifulSoup, NavigableString, Tag\n",
      "import nltk\n",
      "import pandas as pd\n",
      "import re\n",
      "from itertools import islice\n",
      "from collections import Counter\n",
      "import os\n",
      "\n",
      "# Our tools\n",
      "from get_wiki_links import WikiUrlFetch\n",
      "from get_wiki_text import Wiki2Plain"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Get Chapter1.xml and read it into BeautifulSoup"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<h3>TDO: The Discipline of Organizing:</h3>\n",
      "<p>The team chose Prof. Glushko's upcoming textbook <i><a href=\"http://tdo.berkeley.edu/\">The Discipline of Organizing (TDO)</a></i> to test our project. We thought the book would be perfect since it was written with the idea of ultimately linking the book to the entire Web in mind.</p>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ORIGINAL_DRECTORY = '/Users/Desktop/docbookparser/'\n",
      "PATH = 'Chapter1.xml'\n",
      "soup = BeautifulSoup(open(PATH, 'rt').read())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "IOError",
       "evalue": "[Errno 2] No such file or directory: 'Chapter1.xml'",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-4-1a7dcf41982c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mORIGINAL_DRECTORY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/Users/Desktop/docbookparser/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mPATH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Chapter1.xml'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0msoup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[0;31mIOError\u001b[0m: [Errno 2] No such file or directory: 'Chapter1.xml'"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Check which tags are in the soup"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<h3>Picking up tags:</h3>\n",
      "<p>TDO's rich markup under the <a href=\"http://docbook.org/\">DocBook standard</a> provided a solid base for analyzing terms through both tag lookups and natral language-based information extraction.</p>\n",
      "<p>We first looked at the main tags our group looked at were \"keyword,\" \"author,\" \"indexterm,\" \"orgname,\" \"personname,\" and \"phrase.\"</p>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "book_tags = set([tag.name for tag in soup.findAll(True)])\n",
      "print(book_tags)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Get tags and terms and construct a Pandas DataFrame"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# The following script looks for terms within select tags and creates a Pandas DataFrame object.\n",
      "# The DataFrame consists of \"count\" (term counter), \"tag\" (the XML tag for which the term is an attribute),\n",
      "# and \"term\" (the term found within the tag).\n",
      "\n",
      "list_for_df = []\n",
      "tags = ['keyword','author','indexterm','orgname','personname','phrase']\n",
      "for tag in tags:\n",
      "    for i in soup.findAll(tag):\n",
      "        if i.string == None:\n",
      "            list_for_df.append({ 'term':\" \".join([ child.lower().encode('utf-8') for child in i.stripped_strings ]), \\\n",
      "                                'tag': tag, 'count': 1 })\n",
      "        else:\n",
      "            list_for_df.append({ 'term': re.sub('  +','', i.string.lower().encode('utf-8')), 'tag': tag, 'count': 1 })\n",
      "        \n",
      "df = pd.DataFrame(list_for_df)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# First 10 entries of the DataFrame.\n",
      "df[0:10]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Text Processing with nltk to find Named Entities in the Text"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<h3>Regex v. Natural Language Processing:</h3>\n",
      "<p>Our team looked into both Regex-based, and Natural Language-based entity recognition techniques. We initially worked with Regex, thinking it would provide a more flexible, rule-based method of identifying terms with less overhead. We quickly realized that Natural Language Processing offer a better solution to our problem. As the old hacker adage goes: don't reinvent the wheel! We used <a href=\"http://nltk.org/\">NLTK</a>, a Python module rich in natural language tools, to pick up terms surrounded by important tags, and tag each term depending on its part of speech.</p>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<p>Here's the script for cleaning up the text and tagging each word.</p>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#nltk.word_tokenize(string of text)\n",
      "#nltk.pos_tag(list of words)\n",
      "\n",
      "# We used NLTK (Natural Language Toolkit) to identify terms within the text that are not markedup\n",
      "# with XML.\n",
      "\n",
      "# First we create one large text by stripping the markup from the initial XML text.\n",
      "all_xml =  \" \".join([ line for line in soup.stripped_strings])\n",
      "text = nltk.clean_html(all_xml)\n",
      "\n",
      "# We then clean the text through string subsitutions.\n",
      "text = re.sub(r'[\\n\\.,\\(\\)\\?\\!]',' ',text)\n",
      "\n",
      "# Finally, we use nltk.pos_tag to insert part-of-speech tags to each word in the text.\n",
      "tagged_text = nltk.pos_tag(text.split(' '))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<p>This is how we looked for potentially important concepts in the book. We looked for nouns and its variations (singlar v. plural, general nouns v. proper nouns).</p>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pos = ['NN','NNS','NNP','NNPS']\n",
      "\n",
      "terms = []\n",
      "chain = []\n",
      "for term in tagged_text:\n",
      "    \n",
      "    if term[1] in pos:\n",
      "        chain.append(term)\n",
      "        terms.append(\" \".join([ item[0].lower().encode('UTF-8') for item in chain ]))\n",
      "        \n",
      "    else:\n",
      "        if len(chain) > 0:\n",
      "            terms.append(\" \".join([ item[0].lower().encode('UTF-8') for item in chain ]))\n",
      "            chain = []\n",
      "            \n",
      "terms = [ re.sub(' +',' ',term) for term in terms ]\n",
      "terms = [ re.sub('^ ','',term) for term in terms ]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print terms[:10]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Append new terms to DataFrame"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<h3>Connecting to the Web:</h3>\n",
      "<p>Our team decided to connect all identified named entities to the Web via DBpedia. We first attempted to query all terms only to find out it takes too long to fetch every entry on dbpedia. The solution to this was to create a collection.Counter object and filtering terms that occurred more than a certain number of times. We settled for 2 since setting it higher would have excluded many terms crucial to the understanding of a text. The issue here is that this method, while retaining precision, had very high recall.</p>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "terms_for_df = []\n",
      "term_count = Counter(terms)\n",
      "\n",
      "# count_threshold should be a number that returns a good amount of information within a reasonable amount of time.\n",
      "# This depends on what the reader is looking for, and how much computational power they have.\n",
      "# Theoretically speaking, it could be 0 if a reader wants to see all terms, and can process that amount of data on their computer.\n",
      "count_threshold = 2\n",
      "\n",
      "for term,count in term_count.iteritems():\n",
      "    if count >= count_threshold:\n",
      "        terms_for_df.append({ 'term': term,'count': count })\n",
      "        #print term, count"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 47
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 48,
       "text": [
        "<class 'pandas.core.frame.DataFrame'>\n",
        "Int64Index: 366 entries, 0 to 365\n",
        "Data columns:\n",
        "count    366  non-null values\n",
        "tag      366  non-null values\n",
        "term     366  non-null values\n",
        "dtypes: int64(1), object(2)"
       ]
      }
     ],
     "prompt_number": 48
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "list_for_df = []\n",
      "for item in terms_for_df:\n",
      "    if item['term'] != \"\" and item['term'] != \" \":\n",
      "        list_for_df.append({ 'term': item['term'], 'count': item['count'], 'tag': 'raw'})\n",
      "        \n",
      "print list_for_df[0:1]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[{'count': 8, 'term': 'similarity', 'tag': 'raw'}]\n"
       ]
      }
     ],
     "prompt_number": 49
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df = df.append(pd.DataFrame(list_for_df))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 50
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## check the raw terms that were appended\n",
      "df_rawterms = df[df['tag']=='raw']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 51
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df_rawterms"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 52,
       "text": [
        "<class 'pandas.core.frame.DataFrame'>\n",
        "Int64Index: 2557 entries, 0 to 2556\n",
        "Data columns:\n",
        "count    2557  non-null values\n",
        "tag      2557  non-null values\n",
        "term     2557  non-null values\n",
        "dtypes: int64(1), object(2)"
       ]
      }
     ],
     "prompt_number": 52
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 53,
       "text": [
        "<class 'pandas.core.frame.DataFrame'>\n",
        "Int64Index: 2923 entries, 0 to 2556\n",
        "Data columns:\n",
        "count    2923  non-null values\n",
        "tag      2923  non-null values\n",
        "term     2923  non-null values\n",
        "dtypes: int64(1), object(2)"
       ]
      }
     ],
     "prompt_number": 53
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## drop duplicated terms\n",
      "df.drop_duplicates()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 54,
       "text": [
        "<class 'pandas.core.frame.DataFrame'>\n",
        "Int64Index: 2884 entries, 0 to 2556\n",
        "Data columns:\n",
        "count    2884  non-null values\n",
        "tag      2884  non-null values\n",
        "term     2884  non-null values\n",
        "dtypes: int64(1), object(2)"
       ]
      }
     ],
     "prompt_number": 54
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Function to query DBPedia to find a term's wikipedia url"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<h3>DBpedia to Wikipedia:</h3>\n",
      "<p>We tagged each term in the DataFrame as being \"exact\" (exact match), \"good-partial\" (a useful partial match), and \"partial\" (dubious match). We also limited the DBpedia reques to 100 because DBpedia fetching speed was very slow for many terms.</p>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from get_wiki_links import WikiUrlFetch"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 55
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "w = WikiUrlFetch(\"Nunberg, Geoff\")\n",
      "w.results"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 56,
       "text": [
        "[{'match': 'good-partial',\n",
        "  'term': 'geoffrey nunberg',\n",
        "  'url': 'http://dbpedia.org/resource/Geoffrey_Nunberg',\n",
        "  'wiki_url': 'http://en.wikipedia.org/wiki/Geoffrey_Nunberg'}]"
       ]
      }
     ],
     "prompt_number": 56
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from itertools import islice\n",
      "import time\n",
      "\n",
      "new_cols = []\n",
      "\n",
      "for row in islice(df.iterrows(),100): # iterations limited 100 because WikiUrlFetch is very slow for many terms\n",
      "                                        # likely this is because of DBPedia API limitations\n",
      "    r = WikiUrlFetch(row[1]['term'])\n",
      "    wikis = r.results\n",
      "    \n",
      "    for wiki in wikis:\n",
      "        if wiki['match'] != 'none':\n",
      "            new_cols.append( { 'term': row[1]['term'], 'matched_term': wiki['term'], \\\n",
      "              'count': row[1]['count'], 'match': wiki['match'], 'url': wiki['wiki_url'] } )\n",
      "        else:\n",
      "            new_cols.append( { 'term': row[1]['term'], 'matched_term': wiki['term'], \\\n",
      "              'count': row[1]['count'], 'match': wiki['match'], 'url': \"\" } )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": "*"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print len(new_cols)\n",
      "new_cols_df = pd.DataFrame(new_cols)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "187\n"
       ]
      }
     ],
     "prompt_number": 94
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "exact_match = new_cols_df[new_cols_df['match'] == 'exact']\n",
      "exact_match[:10]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>match</th>\n",
        "      <th>matched_term</th>\n",
        "      <th>term</th>\n",
        "      <th>url</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>22 </th>\n",
        "      <td> exact</td>\n",
        "      <td>       metadata</td>\n",
        "      <td>        metadata</td>\n",
        "      <td>       http://en.wikipedia.org/wiki/Metadata</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>87 </th>\n",
        "      <td> exact</td>\n",
        "      <td>     cataloging</td>\n",
        "      <td>      cataloging</td>\n",
        "      <td>     http://en.wikipedia.org/wiki/Cataloging</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>158</th>\n",
        "      <td> exact</td>\n",
        "      <td>     adam smith</td>\n",
        "      <td>     smith, adam</td>\n",
        "      <td>     http://en.wikipedia.org/wiki/Adam_Smith</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>159</th>\n",
        "      <td> exact</td>\n",
        "      <td> charles darwin</td>\n",
        "      <td> darwin, charles</td>\n",
        "      <td> http://en.wikipedia.org/wiki/Charles_Darwin</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>167</th>\n",
        "      <td> exact</td>\n",
        "      <td>           cern</td>\n",
        "      <td>            cern</td>\n",
        "      <td>           http://en.wikipedia.org/wiki/CERN</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
       ],
       "output_type": "pyout",
       "prompt_number": 95,
       "text": [
        "     match    matched_term             term  \\\n",
        "22   exact        metadata         metadata   \n",
        "87   exact      cataloging       cataloging   \n",
        "158  exact      adam smith      smith, adam   \n",
        "159  exact  charles darwin  darwin, charles   \n",
        "167  exact            cern             cern   \n",
        "\n",
        "                                             url  \n",
        "22         http://en.wikipedia.org/wiki/Metadata  \n",
        "87       http://en.wikipedia.org/wiki/Cataloging  \n",
        "158      http://en.wikipedia.org/wiki/Adam_Smith  \n",
        "159  http://en.wikipedia.org/wiki/Charles_Darwin  \n",
        "167            http://en.wikipedia.org/wiki/CERN  "
       ]
      }
     ],
     "prompt_number": 95
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "good_match = new_cols_df[new_cols_df['match'] == 'good-partial']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 96
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "partial_match = new_cols_df[new_cols_df['match'] == 'partial']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 97
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "matched_terms = good_match.append(exact_match)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 108
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "matched_terms[:10]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>match</th>\n",
        "      <th>matched_term</th>\n",
        "      <th>term</th>\n",
        "      <th>url</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>56 </th>\n",
        "      <td> good-partial</td>\n",
        "      <td> geoffrey nunberg</td>\n",
        "      <td>  nunberg, geoff</td>\n",
        "      <td> http://en.wikipedia.org/wiki/Geoffrey_Nunberg</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>106</th>\n",
        "      <td> good-partial</td>\n",
        "      <td>              rai</td>\n",
        "      <td>             uri</td>\n",
        "      <td>              http://en.wikipedia.org/wiki/RAI</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>109</th>\n",
        "      <td> good-partial</td>\n",
        "      <td>              rai</td>\n",
        "      <td>             uri</td>\n",
        "      <td>              http://en.wikipedia.org/wiki/RAI</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>142</th>\n",
        "      <td> good-partial</td>\n",
        "      <td>         data set</td>\n",
        "      <td>         dataset</td>\n",
        "      <td>         http://en.wikipedia.org/wiki/Data_set</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>22 </th>\n",
        "      <td>        exact</td>\n",
        "      <td>         metadata</td>\n",
        "      <td>        metadata</td>\n",
        "      <td>         http://en.wikipedia.org/wiki/Metadata</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>87 </th>\n",
        "      <td>        exact</td>\n",
        "      <td>       cataloging</td>\n",
        "      <td>      cataloging</td>\n",
        "      <td>       http://en.wikipedia.org/wiki/Cataloging</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>158</th>\n",
        "      <td>        exact</td>\n",
        "      <td>       adam smith</td>\n",
        "      <td>     smith, adam</td>\n",
        "      <td>       http://en.wikipedia.org/wiki/Adam_Smith</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>159</th>\n",
        "      <td>        exact</td>\n",
        "      <td>   charles darwin</td>\n",
        "      <td> darwin, charles</td>\n",
        "      <td>   http://en.wikipedia.org/wiki/Charles_Darwin</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>167</th>\n",
        "      <td>        exact</td>\n",
        "      <td>             cern</td>\n",
        "      <td>            cern</td>\n",
        "      <td>             http://en.wikipedia.org/wiki/CERN</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
       ],
       "output_type": "pyout",
       "prompt_number": 109,
       "text": [
        "            match      matched_term             term  \\\n",
        "56   good-partial  geoffrey nunberg   nunberg, geoff   \n",
        "106  good-partial               rai              uri   \n",
        "109  good-partial               rai              uri   \n",
        "142  good-partial          data set          dataset   \n",
        "22          exact          metadata         metadata   \n",
        "87          exact        cataloging       cataloging   \n",
        "158         exact        adam smith      smith, adam   \n",
        "159         exact    charles darwin  darwin, charles   \n",
        "167         exact              cern             cern   \n",
        "\n",
        "                                               url  \n",
        "56   http://en.wikipedia.org/wiki/Geoffrey_Nunberg  \n",
        "106               http://en.wikipedia.org/wiki/RAI  \n",
        "109               http://en.wikipedia.org/wiki/RAI  \n",
        "142          http://en.wikipedia.org/wiki/Data_set  \n",
        "22           http://en.wikipedia.org/wiki/Metadata  \n",
        "87         http://en.wikipedia.org/wiki/Cataloging  \n",
        "158        http://en.wikipedia.org/wiki/Adam_Smith  \n",
        "159    http://en.wikipedia.org/wiki/Charles_Darwin  \n",
        "167              http://en.wikipedia.org/wiki/CERN  "
       ]
      }
     ],
     "prompt_number": 109
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Get the wiki text and image for 'exact' and 'good-partial' matches\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<h3>Linking o Wiki:</h3>\n",
      "<p>We chose \"exact\" and \"good-partial\" matches for Wikipedia linking. The results were not perfect, but very useful. Many of the terms returned correct results (see results below), while some returned incorrect pages, e.g. Radio Televisione Italiana for URI. We scraped the first couple of paragraphs of the Wikipedia page as well as an image if it had one.</p>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_wiki_textimage(url):\n",
      "    wiki = Wiki2Plain(url)\n",
      "    return wiki.text, wiki.image()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 114
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "matched_terms['wiki_text'],matched_terms['wiki_image'] = zip(*matched_terms['url'].apply(get_wiki_textimage))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 112
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "matched_terms[:10]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>match</th>\n",
        "      <th>matched_term</th>\n",
        "      <th>term</th>\n",
        "      <th>url</th>\n",
        "      <th>wiki_text</th>\n",
        "      <th>wiki_image</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>56 </th>\n",
        "      <td> good-partial</td>\n",
        "      <td> geoffrey nunberg</td>\n",
        "      <td>  nunberg, geoff</td>\n",
        "      <td> http://en.wikipedia.org/wiki/Geoffrey_Nunberg</td>\n",
        "      <td> Geoffrey Nunberg (born June, 1945) is an Ameri...</td>\n",
        "      <td> http://simple.wikipedia.org/w/index.php?title=...</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>106</th>\n",
        "      <td> good-partial</td>\n",
        "      <td>              rai</td>\n",
        "      <td>             uri</td>\n",
        "      <td>              http://en.wikipedia.org/wiki/RAI</td>\n",
        "      <td> RAI &mdash; Radiotelevisione Italiana S.p.A. (...</td>\n",
        "      <td> http://simple.wikipedia.org/w/index.php?title=...</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>109</th>\n",
        "      <td> good-partial</td>\n",
        "      <td>              rai</td>\n",
        "      <td>             uri</td>\n",
        "      <td>              http://en.wikipedia.org/wiki/RAI</td>\n",
        "      <td> RAI &mdash; Radiotelevisione Italiana S.p.A. (...</td>\n",
        "      <td> http://simple.wikipedia.org/w/index.php?title=...</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>142</th>\n",
        "      <td> good-partial</td>\n",
        "      <td>         data set</td>\n",
        "      <td>         dataset</td>\n",
        "      <td>         http://en.wikipedia.org/wiki/Data_set</td>\n",
        "      <td> A dataset (or data set) is a collection of dat...</td>\n",
        "      <td>                                              None</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>22 </th>\n",
        "      <td>        exact</td>\n",
        "      <td>         metadata</td>\n",
        "      <td>        metadata</td>\n",
        "      <td>         http://en.wikipedia.org/wiki/Metadata</td>\n",
        "      <td> The term metadata refers to \"data about data\"....</td>\n",
        "      <td>                                              None</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>87 </th>\n",
        "      <td>        exact</td>\n",
        "      <td>       cataloging</td>\n",
        "      <td>      cataloging</td>\n",
        "      <td>       http://en.wikipedia.org/wiki/Cataloging</td>\n",
        "      <td> Cataloging (or cataloguing) is the process of ...</td>\n",
        "      <td>                                              None</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>158</th>\n",
        "      <td>        exact</td>\n",
        "      <td>       adam smith</td>\n",
        "      <td>     smith, adam</td>\n",
        "      <td>       http://en.wikipedia.org/wiki/Adam_Smith</td>\n",
        "      <td> Adam Smith (5 June 1723 OS \u2013 17 July 1790) was...</td>\n",
        "      <td> http://simple.wikipedia.org/w/index.php?title=...</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>159</th>\n",
        "      <td>        exact</td>\n",
        "      <td>   charles darwin</td>\n",
        "      <td> darwin, charles</td>\n",
        "      <td>   http://en.wikipedia.org/wiki/Charles_Darwin</td>\n",
        "      <td> Charles Robert Darwin, FRS (12 February 1809 \u2013...</td>\n",
        "      <td> http://simple.wikipedia.org/w/index.php?title=...</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>167</th>\n",
        "      <td>        exact</td>\n",
        "      <td>             cern</td>\n",
        "      <td>            cern</td>\n",
        "      <td>             http://en.wikipedia.org/wiki/CERN</td>\n",
        "      <td> The European Organization for Nuclear Research...</td>\n",
        "      <td> http://simple.wikipedia.org/w/index.php?title=...</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
       ],
       "output_type": "pyout",
       "prompt_number": 133,
       "text": [
        "            match      matched_term             term  \\\n",
        "56   good-partial  geoffrey nunberg   nunberg, geoff   \n",
        "106  good-partial               rai              uri   \n",
        "109  good-partial               rai              uri   \n",
        "142  good-partial          data set          dataset   \n",
        "22          exact          metadata         metadata   \n",
        "87          exact        cataloging       cataloging   \n",
        "158         exact        adam smith      smith, adam   \n",
        "159         exact    charles darwin  darwin, charles   \n",
        "167         exact              cern             cern   \n",
        "\n",
        "                                               url  \\\n",
        "56   http://en.wikipedia.org/wiki/Geoffrey_Nunberg   \n",
        "106               http://en.wikipedia.org/wiki/RAI   \n",
        "109               http://en.wikipedia.org/wiki/RAI   \n",
        "142          http://en.wikipedia.org/wiki/Data_set   \n",
        "22           http://en.wikipedia.org/wiki/Metadata   \n",
        "87         http://en.wikipedia.org/wiki/Cataloging   \n",
        "158        http://en.wikipedia.org/wiki/Adam_Smith   \n",
        "159    http://en.wikipedia.org/wiki/Charles_Darwin   \n",
        "167              http://en.wikipedia.org/wiki/CERN   \n",
        "\n",
        "                                             wiki_text  \\\n",
        "56   Geoffrey Nunberg (born June, 1945) is an Ameri...   \n",
        "106  RAI &mdash; Radiotelevisione Italiana S.p.A. (...   \n",
        "109  RAI &mdash; Radiotelevisione Italiana S.p.A. (...   \n",
        "142  A dataset (or data set) is a collection of dat...   \n",
        "22   The term metadata refers to \"data about data\"....   \n",
        "87   Cataloging (or cataloguing) is the process of ...   \n",
        "158  Adam Smith (5 June 1723 OS \u2013 17 July 1790) was...   \n",
        "159  Charles Robert Darwin, FRS (12 February 1809 \u2013...   \n",
        "167  The European Organization for Nuclear Research...   \n",
        "\n",
        "                                            wiki_image  \n",
        "56   http://simple.wikipedia.org/w/index.php?title=...  \n",
        "106  http://simple.wikipedia.org/w/index.php?title=...  \n",
        "109  http://simple.wikipedia.org/w/index.php?title=...  \n",
        "142                                               None  \n",
        "22                                                None  \n",
        "87                                                None  \n",
        "158  http://simple.wikipedia.org/w/index.php?title=...  \n",
        "159  http://simple.wikipedia.org/w/index.php?title=...  \n",
        "167  http://simple.wikipedia.org/w/index.php?title=...  "
       ]
      }
     ],
     "prompt_number": 133
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Show our result for Adam Smith (index 158) (first term geoffrey nunberg has issue with image url)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from IPython.core.display import Image\n",
      "print matched_terms.ix[158]['wiki_text']\n",
      "Image(url=matched_terms.ix[158]['wiki_image']) \n",
      "# should limit size, but width=100, height=100 does not work \n",
      "# http://ipython.org/ipython-doc/dev/api/generated/IPython.core.display.html#IPython.core.display.Image"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Adam Smith (5 June 1723 OS \u2013 17 July 1790) was a Scottish moral philosopher and a pioneer of political economy. One of the key figures of the Scottish Enlightenment, Adam Smith is best known for two classic works: The Theory of Moral Sentiments (1759), and An Inquiry into the Nature and Causes of the Wealth of Nations (1776). The latter, usually abbreviated as The Wealth of Nations, is considered his magnum opus and the first modern work of economics. Smith is cited as the \"father of modern economics\" and is still among the most influential thinkers in the field of economics today. In 2009, Smith was named among the \"Greatest Scots\" of all time, in a vote run by Scottish television channel STV.\n",
        "\n",
        "Smith studied social philosophy at the University of Glasgow and at Balliol College in the University of Oxford, where he was one of the first students to benefit from scholarships set up by his fellow Glaswegian John Snell. After graduating, he delivered a successful series of public lectures at the University of Edinburgh, leading him to collaborate with David Hume during the Scottish Enlightenment. Smith obtained a professorship at Glasgow teaching moral philosophy, and during this time he wrote and published The Theory of Moral Sentiments. In his later life, he took a tutoring position that allowed him to travel throughout Europe, where he met other intellectual leaders of his day. Smith then returned home and spent the next ten years writing The Wealth of Nations, publishing it in 1776. He died in 1790 at the age of 67.\n",
        "\n",
        "\n"
       ]
      },
      {
       "html": [
        "<img src=\"http://simple.wikipedia.org/w/index.php?title=Special:FilePath&file=AdamSmith.jpg\" />"
       ],
       "output_type": "pyout",
       "prompt_number": 134,
       "text": [
        "<IPython.core.display.Image at 0x10aa5af90>"
       ]
      }
     ],
     "prompt_number": 134
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Save/Open Data Frame"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## write DataFrame\n",
      "import os\n",
      "path = os.getcwd() + '/wiki_urls_saved.csv'\n",
      "encoding = 'UTF-8'\n",
      "sep = ','\n",
      "matched_terms.to_csv(path,sep,encoding)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 135
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## read DataFrame\n",
      "new_df = pd.read_csv(path)\n",
      "new_df = new_df.drop(['Unnamed: 0'],axis=1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Appendix"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "WikiUrlFetch Class"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#!/usr/bin/env python\n",
      "\n",
      "from urllib2 import urlopen\n",
      "import re\n",
      "import json\n",
      "from nltk import metrics\n",
      "from bs4 import BeautifulSoup, Tag\n",
      "\n",
      "class WikiUrlFetch():\n",
      "\n",
      "    def __init__(self,term):\n",
      "        self.cleaned_term = self.clean_term(term)\n",
      "        self.results = self.get_wiki_url(self.cleaned_term)\n",
      "\n",
      "    def clean_term(self,term):\n",
      "        return re.sub(r\"[^A-Za-z0-9 ]\",\"\",term.lower())\n",
      "        \n",
      "    def check_dbpedia(self, term):\n",
      "        api = 'http://lookup.dbpedia.org/api/search.asmx/KeywordSearch?MaxHits=8&QueryString='\n",
      "        #api = 'http://lookup.dbpedia.org/api/search.asmx/PrefixSearch?MaxHits=10&QueryString='\n",
      "        \n",
      "        try:\n",
      "            response = urlopen(api+term)\n",
      "        except:\n",
      "            return \"\"\n",
      "    \n",
      "        soup = BeautifulSoup(response.read())\n",
      "    \n",
      "        results = []\n",
      "        for result in soup.findAll('result'):\n",
      "            for child in result.children:\n",
      "                if isinstance(child,Tag):\n",
      "                    if child.name == 'label':\n",
      "                        current_label = child.string.lower()\n",
      "                    if child.name == 'uri':\n",
      "                        results.append({ 'term': current_label.encode('utf-8'), 'url': child.string.encode('utf-8') })\n",
      "        #print results[0]\n",
      "        \n",
      "        return self.rank_dbpedia_results(results,term)\n",
      "    \n",
      "    def normalize(self,string):\n",
      "        strings = string.split(\" \")\n",
      "        strings.sort()\n",
      "        return \" \".join(strings)\n",
      "\n",
      "    def rank_dbpedia_results(self,results,term):\n",
      "        \"\"\"\n",
      "        logic:\n",
      "            if edit distance 0, exact match\n",
      "            if edit distance 1-4, good-partial match\n",
      "            if edit distance > 4, partial match (results unsorted)\n",
      "        \"\"\"\n",
      "\n",
      "        matches = []\n",
      "        for result in results:\n",
      "            matches.append([metrics.edit_distance(self.normalize(result['term']), self.normalize(term)), result])\n",
      "\n",
      "        matches.sort()\n",
      "        print matches\n",
      "        if len(matches) == 0:\n",
      "            return [ { 'match': 'none', 'term': term.encode('utf-8') } ] \n",
      "        \n",
      "        elif matches[0][0] == 0:\n",
      "            new_results = [ matches[0][1] ]\n",
      "            new_results[0]['match'] = 'exact'\n",
      "            return new_results\n",
      "\n",
      "        elif matches[0][0] <= 3:\n",
      "            new_results = []\n",
      "            for match in matches:\n",
      "                if match[0] <= 3:\n",
      "                    result = match[1]\n",
      "                    result['match'] = 'good-partial'\n",
      "                    new_results.append(result)\n",
      "            return new_results\n",
      "        \n",
      "        else:\n",
      "            new_results = []\n",
      "            for result in results[0:2]:\n",
      "                result['match'] = 'partial'\n",
      "                new_results.append(result)\n",
      "            return new_results\n",
      "    \n",
      "    def wiki_url(self,url):\n",
      "\n",
      "        term = url[url.rfind('/'):]\n",
      "    \n",
      "        entity_page = 'http://dbpedia.org/data/{}.json'.format(term)\n",
      "    \n",
      "        wiki_type = 'http://xmlns.com/foaf/0.1/primaryTopic'\n",
      "    \n",
      "        try:\n",
      "            response = urlopen(entity_page)\n",
      "        except:\n",
      "            return\n",
      "    \n",
      "        data = json.loads(response.read())\n",
      "        for key,value in data.items():\n",
      "            'http://xmlns.com/foaf/0.1/primaryTopic'\n",
      "            if 'http://xmlns.com/foaf/0.1/primaryTopic' in value:\n",
      "                print key\n",
      "                return key.encode('utf-8')\n",
      "\n",
      "    def get_wiki_url(self, term):\n",
      "    \n",
      "        results = self.check_dbpedia(term)\n",
      "\n",
      "        for result in results:\n",
      "            if result['match'] != 'none':\n",
      "                wiki = self.wiki_url(result['url'])\n",
      "                result['wiki_url'] = wiki\n",
      "\n",
      "        return results"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Wiki2Plain Class"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#!/usr/bin/env python\n",
      "\n",
      "import re\n",
      "import yaml\n",
      "import urllib\n",
      "import urllib2\n",
      "\n",
      "class WikipediaError(Exception):\n",
      "    pass\n",
      "\n",
      "class Wikipedia:\n",
      "    url_article = 'http://%s.wikipedia.org/w/index.php?action=raw&title=%s'\n",
      "    url_image = 'http://%s.wikipedia.org/w/index.php?title=Special:FilePath&file=%s'\n",
      "    url_search = 'http://%s.wikipedia.org/w/api.php?action=query&list=search&srsearch=%s&sroffset=%d&srlimit=%d&format=yaml'\n",
      "    \n",
      "    def __init__(self, lang):\n",
      "        self.lang = lang\n",
      "    \n",
      "    def __fetch(self, url):\n",
      "        request = urllib2.Request(url)\n",
      "        request.add_header('User-Agent', 'Mozilla/5.0')\n",
      "        \n",
      "        try:\n",
      "            result = urllib2.urlopen(request)\n",
      "        except urllib2.HTTPError, e:\n",
      "            raise WikipediaError(e.code)\n",
      "        except urllib2.URLError, e:\n",
      "            raise WikipediaError(e.reason)\n",
      "        \n",
      "        return result\n",
      "    \n",
      "    def article(self, url):\n",
      "        article = url[url.rfind('/')+1:]\n",
      "        url = self.url_article % (self.lang, urllib.quote_plus(article))\n",
      "        content = self.__fetch(url).read()\n",
      "        \n",
      "        if content.upper().startswith('#REDIRECT'):\n",
      "            match = re.match('(?i)#REDIRECT \\[\\[([^\\[\\]]+)\\]\\]', content)\n",
      "            \n",
      "            if not match == None:\n",
      "                return self.article(match.group(1))\n",
      "            \n",
      "            raise WikipediaError('Can\\'t found redirect article.')\n",
      "        \n",
      "        return content\n",
      "    \n",
      "    def image(self, image, thumb=None):\n",
      "        url = self.url_image % (self.lang, image)\n",
      "        result = self.__fetch(url)\n",
      "        content = result.read()\n",
      "        \n",
      "        if thumb:\n",
      "            url = result.geturl() + '/' + thumb + 'px-' + image\n",
      "            url = url.replace('/commons/', '/commons/thumb/')\n",
      "            url = url.replace('/' + self.lang + '/', '/' + self.lang + '/thumb/')\n",
      "            \n",
      "            return self.__fetch(url).read()\n",
      "        \n",
      "        return content\n",
      "    \n",
      "    def search(self, query, page=1, limit=10):\n",
      "        offset = (page - 1) * limit\n",
      "        url = self.url_search % (self.lang, urllib.quote_plus(query), offset, limit)\n",
      "        content = self.__fetch(url).read()\n",
      "        \n",
      "        parsed = yaml.load(content)\n",
      "        search = parsed['query']['search']\n",
      "        \n",
      "        results = []\n",
      "        \n",
      "        if search:\n",
      "            for article in search:\n",
      "                title = article['title'].strip()\n",
      "                \n",
      "                snippet = article['snippet']\n",
      "                snippet = re.sub(r'(?m)<.*?>', '', snippet)\n",
      "                snippet = re.sub(r'\\s+', ' ', snippet)\n",
      "                snippet = snippet.replace(' . ', '. ')\n",
      "                snippet = snippet.replace(' , ', ', ')\n",
      "                snippet = snippet.strip()\n",
      "                \n",
      "                wordcount = article['wordcount']\n",
      "                \n",
      "                results.append({\n",
      "                    'title' : title,\n",
      "                    'snippet' : snippet,\n",
      "                    'wordcount' : wordcount\n",
      "                })\n",
      "        \n",
      "        # yaml.dump(results, default_style='', default_flow_style=False,\n",
      "        #     allow_unicode=True)\n",
      "        return results\n",
      "\n",
      "class Wiki2Plain:\n",
      "#    url_article = 'http://%s.wikipedia.org/w/index.php?action=raw&title=%s'\n",
      "    url_image = 'http://%s.wikipedia.org/w/index.php?title=Special:FilePath&file=%s'\n",
      "#    url_search = 'http://%s.wikipedia.org/w/api.php?action=query&list=search&srsearch=%s&sroffset=%d&srlimit=%d&format=yaml'\n",
      "    \n",
      "    def __init__(self, url):\n",
      "        self.wiki = Wikipedia('en')\n",
      "        self.wiki_article = self.wiki.article(url)\n",
      "\n",
      "        self.text = self.wiki_article\n",
      "        self.text = self.unhtml(self.text)\n",
      "        self.text = self.unwiki(self.text)\n",
      "        self.text = self.punctuate(self.text)\n",
      "        self.text = self.get_summary(self.text)\n",
      "    \n",
      "    def __str__(self):\n",
      "        return self.text\n",
      "    \n",
      "    def unwiki(self, wiki):\n",
      "        \"\"\"\n",
      "        Remove wiki markup from the text.\n",
      "        \"\"\"\n",
      "        wiki = re.sub(r'(?i)\\{\\{IPA(\\-[^\\|\\{\\}]+)*?\\|([^\\|\\{\\}]+)(\\|[^\\{\\}]+)*?\\}\\}', lambda m: m.group(2), wiki)\n",
      "        wiki = re.sub(r'(?i)\\{\\{Lang(\\-[^\\|\\{\\}]+)*?\\|([^\\|\\{\\}]+)(\\|[^\\{\\}]+)*?\\}\\}', lambda m: m.group(2), wiki)\n",
      "        wiki = re.sub(r'\\{\\{[^\\{\\}]+\\}\\}', '', wiki)\n",
      "        wiki = re.sub(r'(?m)\\{\\{[^\\{\\}]+\\}\\}', '', wiki)\n",
      "        wiki = re.sub(r'(?m)\\{\\|[^\\{\\}]*?\\|\\}', '', wiki)\n",
      "        wiki = re.sub(r'(?i)\\[\\[Category:[^\\[\\]]*?\\]\\]', '', wiki)\n",
      "        wiki = re.sub(r'(?i)\\[\\[Image:[^\\[\\]]*?\\]\\]', '', wiki)\n",
      "        wiki = re.sub(r'(?i)\\[\\[File:[^\\[\\]]*?\\]\\]', '', wiki)\n",
      "        wiki = re.sub(r'\\[\\[[^\\[\\]]*?\\|([^\\[\\]]*?)\\]\\]', lambda m: m.group(1), wiki)\n",
      "        wiki = re.sub(r'\\[\\[([^\\[\\]]+?)\\]\\]', lambda m: m.group(1), wiki)\n",
      "        wiki = re.sub(r'\\[\\[([^\\[\\]]+?)\\]\\]', '', wiki)\n",
      "        wiki = re.sub(r'(?i)File:[^\\[\\]]*?', '', wiki)\n",
      "        wiki = re.sub(r'\\[[^\\[\\]]*? ([^\\[\\]]*?)\\]', lambda m: m.group(1), wiki)\n",
      "        wiki = re.sub(r\"''+\", '', wiki)\n",
      "        wiki = re.sub(r'(?m)^\\*$', '', wiki)\n",
      "        \n",
      "        return wiki\n",
      "    \n",
      "    def unhtml(self, html):\n",
      "        \"\"\"\n",
      "        Remove HTML from the text.\n",
      "        \"\"\"\n",
      "        html = re.sub(r'(?i)&nbsp;', ' ', html)\n",
      "        html = re.sub(r'(?i)<br[ \\\\]*?>', '\\n', html)\n",
      "        html = re.sub(r'(?m)<!--.*?--\\s*>', '', html)\n",
      "        html = re.sub(r'(?i)<ref[^>]*>[^>]*<\\/ ?ref>', '', html)\n",
      "        html = re.sub(r'(?m)<.*?>', '', html)\n",
      "        html = re.sub(r'(?i)&amp;', '&', html)\n",
      "        \n",
      "        return html\n",
      "    \n",
      "    def punctuate(self, text):\n",
      "        \"\"\"\n",
      "        Convert every text part into well-formed one-space\n",
      "        separate paragraph.\n",
      "        \"\"\"\n",
      "        text = re.sub(r'\\r\\n|\\n|\\r', '\\n', text)\n",
      "        text = re.sub(r'\\n\\n+', '\\n\\n', text)\n",
      "        \n",
      "        parts = text.split('\\n\\n')\n",
      "        partsParsed = []\n",
      "        \n",
      "        for part in parts:\n",
      "            part = part.strip()\n",
      "            \n",
      "            if len(part) == 0:\n",
      "                continue\n",
      "            \n",
      "            partsParsed.append(part)\n",
      "        \n",
      "        return '\\n\\n'.join(partsParsed)\n",
      "\n",
      "    def get_summary(self,text):\n",
      "        text = text[:text.find('==')]\n",
      "        return text\n",
      "\n",
      "    def image(self):\n",
      "        url_image = 'http://simple.wikipedia.org/w/index.php?title=Special:FilePath&file=' \n",
      "        \"\"\"\n",
      "        Retrieve the first image in the document.\n",
      "        \"\"\"\n",
      "        # match = re.search(r'(?i)\\|?\\s*(image|img|image_flag)\\s*=\\s*(<!--.*-->)?\\s*([^\\\\/:*?<>\"|%]+\\.[^\\\\/:*?<>\"|%]{3,4})', self.wiki)\n",
      "        match = re.search(r'(?i)([^\\\\/:*?<>\"|% =]+)\\.(gif|jpg|jpeg|png|bmp)', self.wiki_article)\n",
      "        \n",
      "        if match:\n",
      "            return url_image + '%s.%s' % match.groups()\n",
      "        \n",
      "        return None"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}